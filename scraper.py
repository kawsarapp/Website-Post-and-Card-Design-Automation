import sys
import json
import io
import re
from urllib.parse import urljoin
import trafilatura
from curl_cffi import requests
from bs4 import BeautifulSoup

# ==========================================
# üî• UNIVERSAL ENCODING FIX (Windows/Linux)
# ==========================================
# Windows ‡¶ï‡¶®‡¶∏‡ßã‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶ó‡ßá‡¶≤‡ßá ‡¶ï‡ßç‡¶∞‡¶æ‡¶∂ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶æ‡¶á ‡¶è‡¶ü‡¶æ ‡¶´‡¶ø‡¶ï‡ßç‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶≤‡ßã‡•§
if sys.platform.startswith('win'):
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
else:
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶Ü‡¶∞‡ßç‡¶ó‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶ö‡ßá‡¶ï
try:
    target_url = sys.argv[1]
except IndexError:
    print(json.dumps({"error": "No URL provided"}))
    sys.exit(1)

# ==========================================
# üöÄ FAST REQUEST (Browser Impersonation)
# ==========================================
def get_html(url):
    try:
        # ‡¶≤‡ßá‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡ßç‡¶∞‡ßã‡¶Æ ‡¶¨‡ßç‡¶∞‡¶æ‡¶â‡¶ú‡¶æ‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶Ü‡¶ö‡¶∞‡¶£ ‡¶ï‡¶∞‡¶¨‡ßá
        response = requests.get(
            url,
            impersonate="chrome120", 
            timeout=30,
            headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'bn-BD,bn;q=0.9,en-US;q=0.8,en;q=0.7',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
        )
        if response.status_code == 200:
            # ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç ‡¶Ö‡¶ü‡ßã-‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
            if response.encoding is None or response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
            return response.text
    except Exception as e:
        # ‡¶∏‡¶æ‡¶á‡¶≤‡ßá‡¶®‡ßç‡¶ü ‡¶´‡ßá‡¶á‡¶≤, ‡¶Ø‡¶æ‡¶§‡ßá PHP ‡¶™‡¶∞‡ßá‡¶∞ ‡¶Æ‡ßá‡¶•‡¶° ‡¶ü‡ßç‡¶∞‡¶æ‡¶á ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá
        pass
    return None

# ==========================================
# üßπ SMART CLEANER (Garbage Removal)
# ==========================================
def clean_html(soup):
    # ‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶ï‡¶∞‡¶æ
    for tag in soup(['script', 'style', 'iframe', 'nav', 'footer', 'header', 'form', 'svg', 'noscript']):
        tag.decompose()

    # ‡¶ï‡¶Æ‡¶® ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶∏ ‡¶è‡¶¨‡¶Ç ‡¶ó‡¶æ‡¶∞‡ßç‡¶¨‡ßá‡¶ú ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶ï‡¶∞‡¶æ
    garbage_selectors = [
        '.advertisement', '.ads', '.ad-container', '.social-share', 
        '.share-buttons', '.related-news', '.read-more', '.tags', 
        '.author-bio', '.sidebar', '.comments', '.meta-info', 
        '[class*="taboola"]', '[id*="taboola"]', '[class*="popup"]'
    ]
    
    for selector in garbage_selectors:
        for tag in soup.select(selector):
            tag.decompose()
            
    return soup

# ==========================================
# üß† INTELLIGENT EXTRACTION
# ==========================================
def extract_data(html, base_url):
    soup = BeautifulSoup(html, 'html.parser')
    
    # ‡ßß. ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶®
    title = ""
    if soup.find('h1'):
        title = soup.find('h1').get_text(strip=True)
    elif soup.title:
        title = soup.title.string

    # ‡ß®. JSON-LD (Schema.org) ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡¶æ‡¶ü‡¶æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ (‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡ßÅ‡¶≤)
    image = None
    schema_body = None
    
    ld_json = soup.find_all('script', type='application/ld+json')
    for script in ld_json:
        try:
            data = json.loads(script.string)
            # ‡¶ó‡ßç‡¶∞‡¶æ‡¶´ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá‡¶≤ ‡¶ï‡¶∞‡¶æ
            if '@graph' in data:
                for item in data['@graph']:
                    if item.get('@type') in ['NewsArticle', 'Article', 'BlogPosting']:
                        data = item
                        break
            
            # ‡¶á‡¶Æ‡ßá‡¶ú ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
            if 'image' in data:
                img_data = data['image']
                if isinstance(img_data, dict):
                    image = img_data.get('url')
                elif isinstance(img_data, list):
                    image = img_data[0]
                elif isinstance(img_data, str):
                    image = img_data
            
            # ‡¶¨‡¶°‡¶ø ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
            if 'articleBody' in data:
                schema_body = data['articleBody']
                
        except:
            pass

    # ‡ß©. ‡¶á‡¶Æ‡ßá‡¶ú ‡¶´‡¶≤‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï (‡¶Ø‡¶¶‡¶ø JSON-LD ‡¶§‡ßá ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá)
    if not image:
        # ‡¶ì‡¶™‡ßá‡¶® ‡¶ó‡ßç‡¶∞‡¶æ‡¶´ ‡¶á‡¶Æ‡ßá‡¶ú
        og_image = soup.find('meta', property='og:image')
        if og_image:
            image = og_image.get('content')
        else:
            # ‡¶Æ‡ßá‡¶á‡¶® ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶∞‡¶ø‡ßü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶á‡¶Æ‡ßá‡¶ú ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
            main_area = soup.select_one('article, [itemprop="articleBody"], .post-content, #content, .details')
            target = main_area if main_area else soup
            
            for img in target.find_all('img'):
                src = img.get('src') or img.get('data-src')
                # ‡¶≤‡ßã‡¶ó‡ßã ‡¶¨‡¶æ ‡¶õ‡ßã‡¶ü ‡¶Ü‡¶á‡¶ï‡¶® ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ
                if src and 'logo' not in src.lower() and 'icon' not in src.lower():
                    # ‡¶∞‡¶ø‡¶≤‡ßá‡¶ü‡¶ø‡¶≠ ‡¶™‡¶æ‡¶• ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ
                    image = urljoin(base_url, src)
                    break

    # ‡ß™. ‡¶¨‡¶°‡¶ø ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® (Trafilatura - ‡¶∏‡ßá‡¶∞‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ï‡ßç‡¶≤‡¶ø‡¶®‡¶æ‡¶∞)
    # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶∏‡ßç‡¶Ø‡ßÅ‡¶™ ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ï‡¶∞‡¶æ
    clean_soup = clean_html(soup)
    cleaned_html_str = str(clean_soup)
    
    body_text = trafilatura.extract(
        cleaned_html_str, 
        include_images=False, 
        include_comments=False, 
        favor_precision=True,
        target_language='bn' # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶™‡¶ü‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú‡¶°
    )
    
    # Trafilatura ‡¶´‡ßá‡¶á‡¶≤ ‡¶ï‡¶∞‡¶≤‡ßá ‡¶´‡¶≤‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï (Schema Body ‡¶Ö‡¶•‡¶¨‡¶æ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶´)
    if not body_text:
        if schema_body:
            body_text = schema_body
        else:
            # ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡ßü‡¶æ‡¶≤ ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶´ ‡¶ú‡ßü‡ßá‡¶®
            paragraphs = clean_soup.find_all('p')
            body_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 40])

    # ‡ß´. HTML ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡¶ø‡¶Ç (Line break to <p>)
    formatted_body = ""
    if body_text:
        # ‡¶≤‡¶æ‡¶á‡¶® ‡¶¨‡ßç‡¶∞‡ßá‡¶ï ‡¶¶‡¶ø‡ßü‡ßá ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶´ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶∞‡¶æ
        for para in body_text.split('\n'):
            clean_para = para.strip()
            if len(clean_para) > 10:
                formatted_body += f"<p>{clean_para}</p>"

    return {
        "title": title,
        "body": formatted_body,
        "image": image,
        "source_url": base_url
    }

# ==========================================
# üèÅ MAIN EXECUTION
# ==========================================
try:
    html_content = get_html(target_url)
    
    if html_content:
        data = extract_data(html_content, target_url)
        
        # ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶∂‡¶®: ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ ‡¶¨‡¶æ ‡¶¨‡¶°‡¶ø ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶è‡¶∞‡¶∞
        if data['title'] and data['body']:
            print(json.dumps(data, ensure_ascii=False))
        else:
            # ‡¶°‡¶æ‡¶ü‡¶æ ‡¶®‡¶æ ‡¶™‡ßá‡¶≤‡ßá ‡¶è‡¶Æ‡ßç‡¶™‡¶ü‡¶ø ‡¶ú‡ßá‡¶∏‡¶®, ‡¶Ø‡¶æ‡¶§‡ßá PHP ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶∏‡ßç‡¶ü‡ßá‡¶™‡ßá ‡¶Ø‡¶æ‡ßü
            print(json.dumps({"error": "Content extraction failed"}))
    else:
        print(json.dumps({"error": "Failed to retrieve HTML"}))

except Exception as e:
    # ‡¶Ø‡ßá‡¶ï‡ßã‡¶® ‡¶ï‡ßç‡¶∞‡¶ø‡¶ü‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶è‡¶∞‡¶∞‡ßá JSON ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶®
    print(json.dumps({"error": str(e)}))